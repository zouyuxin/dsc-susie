# dsc-susie

## Goal

This is a Dynamic Statistical Comparison for testing SuSiE performance given a design matrix `X` from real data, e.g. GTEx, single-cell, or GSEA data.

Our intention is to investigate if SuSiE has a good interpretation (i.e. high power, low false discovery rate) in the context of different X structures.

## Example

The example uses GTEx data from two tissues: whole blood and brain cells (source:link...). In other words, design matrix `X` is real genotype data from 755 whole blood and 439 brain cell samples on 1000 genes. Then we simulate a gaussian `y` and a binary `y` under various circumstances for different questions of interest. For instance, we set different numbers of nonzero effects in the simulation to investigate whether SuSiE has a better performance when the number of effects is sparse. 

## Performance evaluation criteria

We evaluate SuSiE performance by the following four numerical values:

  - power: 
          \[ \frac{\text{# of confidence sets that contain at least one true effect}}{\text{# of true effects}}\]
  
  - false discovery rate (fdr):
          \[ 1 - \frac{\text{# of confidence sets that contain at least one true effect}}{\text{# of confidence sets}}\]
  
  - median size of confidence sets
  - top hit rate: a top hit means that when a confidence set contains a true effect, if the true effect has the highest posterior inclusion probability(PIP), we count it as a top hit. 
    top hit rate = # of top hit / # of confidence sets

## Questions to investigate

### Gaussian SuSiE

Suppose we simulate 10 datasets with gaussian `y` given a design matrix `X` from GTEx data. 

#### How is power?
When first with SuSiE, we want to know how is power of this method?

  - Suppose we have one nonzero effect. How is SuSiE's power under various PVEs (proportion variance explained)? (See powerVSpve.ipynb)
  
#### How to set a prior?
After learning SuSiE's power for given real data, we want to give suggestions on setting priors in SuSiE by asking the following questions: 

  - What is SuSiE performance when a simulated pve is very small while a prior is very large? (See pveVSprior1.ipynb)
  - How to pick prior when pve is moderate? (See pveVSprior2.ipynb)
  - What is SuSiE performance when a simulated pve is very large while a prior is very small? (See pveVSprior3.ipynb)
  
#### Number of effects
  - What is the SuSiE performance when the number of nonzero effects is 1, 5, 10, or 20?
  - What is the SuSiE performance when the number of nonzero effects is as large as 200? Will the confidence sets produced by SuSiE be still sparse?
  

#### L0 initialization
  - What is the SuSiE performance when SuSiE is initialized with L0Learn? Will the number of iteration be smaller? 
  - Will the duplication problem (i.e SuSiE produces exactly same confidence sets) be solved by using L0 initialization?

### Binary SuSiE
Suppose we simulate a binary `y` given a GTEx design matrix `X`. 

#### Number of effects
  - What is the SuSiE performance when the number of nonzero effects is 1, 5, 10, or 20?
  - What is the SuSiE performance when the number of nonzero effects is as large as 200? Will the confidence sets produced by SuSiE be still sparse?
  - Is there a performance difference if we simulate a gaussian `y` versus a binary `y` when the number of effects is default 10?
  
## Input `X`

To input a customized design matrix `X`, please go to `benchmark.dsc`. At the bottom, change the `pathX` to the path of your matrix `X` as you want. 

## Run DSC

The main DSC file is `benchmarks.dsc`. To see what is available:

```
./benchmark.dsc -h
```

before running, 
```
rm -Rf benchmark.scripts.html benchmark.html benchmark

```

and to run the benchmark:

```
dsc benchmark.dsc --replicate 10
```

Or to run a minimal test benchmark, e.g.
```
dsc benchmark.dsc --truncate
```

